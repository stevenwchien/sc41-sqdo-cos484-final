{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch \n",
    "import transformers as ppb\n",
    "import time\n",
    "from scipy import spatial\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from feature_engineering import refuting_features, polarity_features, hand_features, gen_or_load_feats\n",
    "from feature_engineering import word_overlap_features\n",
    "from utils.dataset import DataSet\n",
    "from utils.generate_test_splits import kfold_split, get_stances_for_folds\n",
    "from utils.score import report_score, LABELS, score_submission\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.system import parse_params, check_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_or_load_feats(feat_fn, headlines, bodies, feature_file, tokenizer, model):\n",
    "    if not os.path.isfile(feature_file):\n",
    "        feats = feat_fn(headlines, bodies, tokenizer, model)\n",
    "        np.save(feature_file, feats)\n",
    "    return np.load(feature_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMaxSentenceLength(d):\n",
    "    maxLength = 0\n",
    "    maxSent = -1\n",
    "    articleNum = -1\n",
    "    count = 0\n",
    "    for articles in d.articles:\n",
    "        article = d.articles[articles]\n",
    "        split_list = article.split(\".\")\n",
    "        clean_list = [clean(i) for i in split_list]\n",
    "        for i in clean_list:\n",
    "            words = i.split(\" \")\n",
    "            if len(words) > maxLength:\n",
    "                maxLength = len(words) \n",
    "                maxSent = words\n",
    "                articleNum = articles\n",
    "            if len(i) > 512:\n",
    "                count+=1\n",
    "        \n",
    "    print(maxLength)\n",
    "    print(maxSent)\n",
    "    print(articleNum)\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(embeddings):     \n",
    "    max_len = MAX_LENGTH\n",
    "    for i in embeddings:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in embeddings])\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_features(headlines, bodies, tokenizer, model):\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        start_time = time.time()\n",
    "        split_headline = headline.split(\".\")\n",
    "        split_body = body.split(\".\")\n",
    "        # Splitting Headline and Body into seperate sentences\n",
    "        headline_sentences = []\n",
    "        body_sentences     = []\n",
    "        for i in range(len(split_headline)):\n",
    "            clean_headline = clean(split_headline[i])\n",
    "            words = clean_headline.split(\" \")\n",
    "            words = words[0:MAX_LENGTH]\n",
    "            headline_sentences.append(words)\n",
    "        for i in range(len(split_body)):\n",
    "            clean_body = clean(split_body[i])\n",
    "            words = clean_body.split(\" \")\n",
    "            if len(words) > MAX_LENGTH:\n",
    "                words = words[0:MAX_LENGTH]\n",
    "            body_sentences.append(words)\n",
    "            \n",
    "        # Creating Embeds for each sentences\n",
    "        headline_tokens = []\n",
    "        body_tokens     = [] \n",
    "        for i in headline_sentences:\n",
    "            encoded = tokenizer.encode(i, add_special_tokens = True)\n",
    "            headline_tokens.append(encoded)\n",
    "        for i in body_sentences:\n",
    "            encoded = tokenizer.encode(i, add_special_tokens = True)\n",
    "            body_tokens.append(encoded)\n",
    "       \n",
    "        # Padding Tokens\n",
    "        padded_headline_embeds = pad(headline_tokens)\n",
    "        padded_body_embeds     = pad(body_tokens)\n",
    "        \n",
    "        # np.array checks:\n",
    "        # print(\"Shape of Headline Padded: \", np.array(padded_headline_embeds).shape)\n",
    "        # print(\"Shape of Body Padded: \", np.array(padded_body_embeds).shape)\n",
    "\n",
    "        # Preparing for input into Models\n",
    "        headline_input = torch.LongTensor(np.array(padded_headline_embeds))\n",
    "        headline_attention_mask = np.where(padded_headline_embeds != 0, 1, 0)\n",
    "        headline_attention_mask = torch.tensor(headline_attention_mask)\n",
    "        \n",
    "        body_input     = torch.LongTensor(np.array(padded_body_embeds))\n",
    "        body_attention_mask = np.where(padded_body_embeds != 0, 1, 0)\n",
    "        body_attention_mask = torch.tensor(body_attention_mask)\n",
    "        \n",
    "        # Applying to BERT Model to get Embeddings\n",
    "        with torch.no_grad():\n",
    "            last_headline_hidden_states = model(headline_input, attention_mask=headline_attention_mask)\n",
    "            last_body_hidden_states     = model(body_input, attention_mask=body_attention_mask)\n",
    "        \n",
    "        headline_features = last_headline_hidden_states[0][:,0,:].numpy()\n",
    "        body_features = last_body_hidden_states[0][:,0,:].numpy()\n",
    "\n",
    "        # Average embeddings for all the sentences\n",
    "        h_average_features = np.expand_dims(np.average(headline_features, axis=0),axis=0)\n",
    "        b_average_features = np.expand_dims(np.average(body_features, axis=0), axis=0)\n",
    "        \n",
    "        # Cosine similarity and Maximum similarities\n",
    "        cosine_sim = np.array(spatial.distance.cosine(h_average_features, b_average_features), ndmin=2)\n",
    "        bert_feature = np.concatenate((h_average_features, b_average_features), axis=1)\n",
    "        bert_feature = np.concatenate((bert_feature, cosine_sim), axis = 1)\n",
    "        # Creating Bert Features\n",
    "        # print(\"Bert Features Shape: \", bert_feature.shape)\n",
    "        X.append(bert_feature)     \n",
    "        print(\"Data Loaded in {} second\".format(time.time() - start_time))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_with_tokenizer(stances,dataset,name, tokenizer, model):\n",
    "    h, b, y = [],[],[]\n",
    "\n",
    "    for stance in stances:\n",
    "        y.append(LABELS.index(stance['Stance']))\n",
    "        h.append(stance['Headline'])\n",
    "        b.append(dataset.articles[stance['Body ID']])\n",
    "    X_bert    = gen_or_load_feats(bert_features, h, b, \"features/bert_features.\"+name+\".npy\", tokenizer, model)\n",
    "    X = np.c_[X_bert]\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "9622\n",
      "4663\n",
      "4039\n",
      "3644\n",
      "4273\n",
      "3944\n",
      "3388\n",
      "4124\n",
      "3783\n",
      "4644\n",
      "3848\n"
     ]
    }
   ],
   "source": [
    "d = DataSet()\n",
    "with open('holdout.json', 'r') as read_file:\n",
    "    hold_out_stances = json.loads(read_file.read())\n",
    "    print(len(hold_out_stances))\n",
    "fold_stances=[]\n",
    "for i in range(10):\n",
    "    with open('fold'+str(i)+'.json', 'r') as read_file:\n",
    "        fold = json.loads(read_file.read())\n",
    "        fold_stances.append(fold)\n",
    "        print(len(fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertTokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "bert_model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4124, 1537)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold=6\n",
    "Xs[fold], ys[fold] = generate_features_with_tokenizer(fold_stances[fold],d,str(fold), bertTokenizer, bert_model)\n",
    "Xs[fold].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "6\n",
      "0\n",
      "7\n",
      "5\n",
      "2\n",
      "8\n",
      "9\n",
      "3\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "d = DataSet()\n",
    "folds,hold_out = kfold_split(d,n_folds=10)\n",
    "fold_stances, hold_out_stances = get_stances_for_folds(d,folds,hold_out)\n",
    "\n",
    "Xs = dict()\n",
    "ys = dict()\n",
    "\n",
    "# Load/Precompute all features now\n",
    "X_holdout,y_holdout = generate_features_with_tokenizer(hold_out_stances,d,\"holdout\", bertTokenizer, bert_model)\n",
    "for fold in fold_stances:\n",
    "    print(fold)\n",
    "    Xs[fold],ys[fold] = generate_features_with_tokenizer(fold_stances[fold],d,str(fold), bertTokenizer, bert_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = nn.Linear(1537, 600)\n",
    "        self.hidden2 = nn.Linear(600, 600)\n",
    "        self.hidden3 = nn.Linear(600, 600)\n",
    "        self.output = nn.Linear(600, 4)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.hidden2(F.relu(x))\n",
    "        x = self.hidden3(F.relu(x))\n",
    "        x = self.output(F.relu(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1] loss: 1.384\n",
      "[Epoch: 2] loss: 1.383\n",
      "[Epoch: 3] loss: 1.381\n",
      "[Epoch: 4] loss: 1.379\n",
      "[Epoch: 5] loss: 1.376\n",
      "[Epoch: 6] loss: 1.374\n",
      "[Epoch: 7] loss: 1.372\n",
      "[Epoch: 8] loss: 1.369\n",
      "[Epoch: 9] loss: 1.367\n",
      "[Epoch: 10] loss: 1.364\n",
      "[Epoch: 11] loss: 1.362\n",
      "[Epoch: 12] loss: 1.359\n",
      "[Epoch: 13] loss: 1.357\n",
      "[Epoch: 14] loss: 1.355\n",
      "[Epoch: 15] loss: 1.352\n",
      "[Epoch: 16] loss: 1.350\n",
      "[Epoch: 17] loss: 1.348\n",
      "[Epoch: 18] loss: 1.345\n",
      "[Epoch: 19] loss: 1.343\n",
      "[Epoch: 20] loss: 1.341\n",
      "[Epoch: 21] loss: 1.338\n",
      "[Epoch: 22] loss: 1.336\n",
      "[Epoch: 23] loss: 1.334\n",
      "[Epoch: 24] loss: 1.331\n",
      "[Epoch: 25] loss: 1.329\n",
      "[Epoch: 26] loss: 1.327\n",
      "[Epoch: 27] loss: 1.325\n",
      "[Epoch: 28] loss: 1.322\n",
      "[Epoch: 29] loss: 1.320\n",
      "[Epoch: 30] loss: 1.318\n",
      "[Epoch: 31] loss: 1.315\n",
      "[Epoch: 32] loss: 1.313\n",
      "[Epoch: 33] loss: 1.311\n",
      "[Epoch: 34] loss: 1.308\n",
      "[Epoch: 35] loss: 1.306\n",
      "[Epoch: 36] loss: 1.304\n",
      "[Epoch: 37] loss: 1.302\n",
      "[Epoch: 38] loss: 1.299\n",
      "[Epoch: 39] loss: 1.297\n",
      "[Epoch: 40] loss: 1.295\n",
      "[Epoch: 41] loss: 1.292\n",
      "[Epoch: 42] loss: 1.290\n",
      "[Epoch: 43] loss: 1.288\n",
      "[Epoch: 44] loss: 1.285\n",
      "[Epoch: 45] loss: 1.283\n",
      "[Epoch: 46] loss: 1.281\n",
      "[Epoch: 47] loss: 1.278\n",
      "[Epoch: 48] loss: 1.276\n",
      "[Epoch: 49] loss: 1.274\n",
      "[Epoch: 50] loss: 1.271\n",
      "[Epoch: 51] loss: 1.269\n",
      "[Epoch: 52] loss: 1.267\n",
      "[Epoch: 53] loss: 1.264\n",
      "[Epoch: 54] loss: 1.262\n",
      "[Epoch: 55] loss: 1.260\n",
      "[Epoch: 56] loss: 1.257\n",
      "[Epoch: 57] loss: 1.255\n",
      "[Epoch: 58] loss: 1.253\n",
      "[Epoch: 59] loss: 1.250\n",
      "[Epoch: 60] loss: 1.248\n",
      "[Epoch: 61] loss: 1.246\n",
      "[Epoch: 62] loss: 1.243\n",
      "[Epoch: 63] loss: 1.241\n",
      "[Epoch: 64] loss: 1.238\n",
      "[Epoch: 65] loss: 1.236\n",
      "[Epoch: 66] loss: 1.234\n",
      "[Epoch: 67] loss: 1.231\n",
      "[Epoch: 68] loss: 1.229\n",
      "[Epoch: 69] loss: 1.226\n",
      "[Epoch: 70] loss: 1.224\n",
      "[Epoch: 71] loss: 1.221\n",
      "[Epoch: 72] loss: 1.219\n",
      "[Epoch: 73] loss: 1.217\n",
      "[Epoch: 74] loss: 1.214\n",
      "[Epoch: 75] loss: 1.212\n",
      "[Epoch: 76] loss: 1.209\n",
      "[Epoch: 77] loss: 1.207\n",
      "[Epoch: 78] loss: 1.204\n",
      "[Epoch: 79] loss: 1.202\n",
      "[Epoch: 80] loss: 1.199\n",
      "[Epoch: 81] loss: 1.197\n",
      "[Epoch: 82] loss: 1.194\n",
      "[Epoch: 83] loss: 1.192\n",
      "[Epoch: 84] loss: 1.189\n",
      "[Epoch: 85] loss: 1.187\n",
      "[Epoch: 86] loss: 1.184\n",
      "[Epoch: 87] loss: 1.182\n",
      "[Epoch: 88] loss: 1.179\n",
      "[Epoch: 89] loss: 1.176\n",
      "[Epoch: 90] loss: 1.174\n",
      "[Epoch: 91] loss: 1.171\n",
      "[Epoch: 92] loss: 1.169\n",
      "[Epoch: 93] loss: 1.166\n",
      "[Epoch: 94] loss: 1.164\n",
      "[Epoch: 95] loss: 1.161\n",
      "[Epoch: 96] loss: 1.158\n",
      "[Epoch: 97] loss: 1.156\n",
      "[Epoch: 98] loss: 1.153\n",
      "[Epoch: 99] loss: 1.150\n",
      "[Epoch: 100] loss: 1.148\n",
      "Score for fold 6 was - 0.4564780235564493\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-53855b6890cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Score for fold \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" was - \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mbest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mbest_fold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_score' is not defined"
     ]
    }
   ],
   "source": [
    "#test variable batch size\n",
    "# batch_size = 50\n",
    "epochs = 100\n",
    "track_loss = []\n",
    "for fold in fold_stances:\n",
    "    ids = list(range(len(folds)))\n",
    "    del ids[fold]\n",
    "        \n",
    "    X_train = np.vstack(tuple([Xs[i] for i in ids]))\n",
    "    y_train = np.hstack(tuple([ys[i] for i in ids]))\n",
    "    \n",
    "    X_test = Xs[fold]\n",
    "    y_test = ys[fold]\n",
    "\n",
    "    inputs = torch.FloatTensor(X_train)\n",
    "    labels = torch.LongTensor(y_train)\n",
    "    \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # print(labels)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        # print(out.size())\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        track_loss.append(running_loss)\n",
    "        print('[Epoch: %d] loss: %.3f' % (epoch + 1, running_loss))\n",
    "    \n",
    "    outputs = net(torch.FloatTensor(X_test))\n",
    "    _, o_predicted = torch.max(outputs, 1)\n",
    "    o_predicted = o_predicted.tolist()\n",
    "    predicted = [LABELS[int(a)] for a in o_predicted]\n",
    "    actual = [LABELS[int(a)] for a in y_test]\n",
    "    fold_score, _ = score_submission(actual, predicted)\n",
    "    max_fold_score, _ = score_submission(actual, actual)\n",
    "\n",
    "    score = fold_score/max_fold_score\n",
    "\n",
    "    print(\"Score for fold \"+ str(fold) + \" was - \" + str(score))\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_fold = clf\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on the dev set\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |     0     |     0     |     0     |    762    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |     0     |     0     |     0     |    162    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |     0     |     0     |     0     |   1800    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     0     |     0     |     0     |   6898    |\n",
      "-------------------------------------------------------------\n",
      "Score: 1724.5 out of 4448.5\t(38.765876138024055%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38.765876138024055"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = net(torch.FloatTensor(X_holdout))\n",
    "_, o_predicted = torch.max(outputs, 1)\n",
    "o_predicted = o_predicted.tolist()\n",
    "predicted = [LABELS[int(a)] for a in o_predicted]\n",
    "actual = [LABELS[int(a)] for a in y_holdout]\n",
    "print(\"Scores on the dev set\")\n",
    "report_score(actual,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3, 3,  ..., 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "_, o_predicted = torch.max(outputs, 1)\n",
    "print(o_predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
